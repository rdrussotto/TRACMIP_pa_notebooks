{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script to calculate Gregory regressions on the TRACMIP output (zonal mean of Aqua4xCO2 - AquaControl). \n",
    "#This is necessary to obtain forcing and feedback estimates that can be placed into the energy balance model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/anaconda3/envs/pangeo3.7/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/usr/local/python/anaconda3/envs/pangeo3.7/lib/python3.6/site-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import voigtColors as vc\n",
    "voigtColors = vc.voigtColors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How am I going to do this? \n",
    "\n",
    "#Need the monthly mean output from the beginning to the end of the AquaControl simulations. \n",
    "#Specifically need OLR and upward and downward SW radiation at TOA (also \"rldt\" for models that have it). \n",
    "#Also need surface temperature. \n",
    "#And for breaking down the different forcings and feedbacks, may need all the other TOA & surface energy variables as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test loading a file from Michela's climatology: (based on Charles's script)\n",
    "ds_test = xr.open_dataset('http://fletcher.ldeo.columbia.edu:81/home/.OTHER/.biasutti/.netcdf/.TRACMIP/.AmonClimAug2nd2016/.PP/.ECHAM-6.3/.AquaControl/.hfls_tf.nc/.hfls/dods', decode_times=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As I suspected this only has 12 times--time averaging has already been done by Michela. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will instead need to get the data some other way, most likely downloading the Amon files from ESGF.\n",
    "#I think I have no other choice... Miami repository has the data all in one NetCDF file, for some models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5-14-19: have now downloaded the data (in ../nc_esgf/Amon). Need to figure out how to load everything from the XArray files\n",
    "#Used Glob for the previous EBM tuning. \n",
    "#How about creating a dataset object that contains the zonal means of all the relevant variables for the regressions/feedbacks? \n",
    "#(Maintaining monthly values but this will make regressions much easier). \n",
    "#Another one for global means perhaps. \n",
    "#This way I only have to open the individual model NetCDF files once. \n",
    "#But... there is an issue. The data lengths are not the same for each file/model. \n",
    "#So, separate dataset for each model then? Don't want to truncate each one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load climatology\n",
    "ds_climo = xr.open_dataset('../nc_revised_20181130/master.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filenames = {'AM2': 'AM21', \n",
    "                   'CAM3': 'CAM3', \n",
    "                   'CAM4': 'CAM4',\n",
    "                   'CNRM-AM6-DIA-v2': 'CNRM-AM5', \n",
    "                   'ECHAM-6.1': 'ECHAM61', \n",
    "                   'ECHAM-6.3': 'ECHAM63', \n",
    "                   'GISS-ModelE2': 'GISS-ModelE2',\n",
    "                   'IPSL-CM5A': 'LMDZ5A', \n",
    "                   'MIROC5': 'MIROC5', \n",
    "                   'MPAS': 'MPAS', \n",
    "                   'MetUM-GA6-CTL': 'MetUM-CTL', \n",
    "                   'MetUM-GA6-ENT': 'MetUM-ENT', \n",
    "                   'NorESM2': 'CAM5Nor'}\n",
    "\n",
    "models = ['AM2', 'CAM3', 'CAM4', 'CNRM-AM6-DIA-v2', 'ECHAM-6.1', 'ECHAM-6.3', 'GISS-ModelE2', 'IPSL-CM5A', \n",
    "              'MIROC5', 'MPAS', 'MetUM-GA6-CTL', 'MetUM-GA6-ENT', 'NorESM2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through the models, load the monthly means for all the relevant variables, take zonal mean, regrid and save as a separate NetCDF file\n",
    "#For Aqua4xCO2. \n",
    "#Note rsdtcs doesn't exist.\n",
    "#rluscs doesn't either (on repository) but I only need SW variables for APRP.\n",
    "varlist = ['ts', 'clt', 'rlds', 'rsds', 'rlus', 'rsus', 'rsdt', 'rsut', 'rlut', 'rldscs', 'rsdscs', 'rsuscs', 'rsutcs', 'rlutcs']\n",
    "varlist_no_rldscs = ['ts', 'clt', 'rlds', 'rsds', 'rlus', 'rsus', 'rsdt', 'rsut', 'rlut', 'rsdscs', 'rsuscs', 'rsutcs', 'rlutcs'] #NorESM is missing this\n",
    "#for model in models:\n",
    "#for model in ['NorESM2']:\n",
    "for model in ['ECHAM-6.1', 'ECHAM-6.3']:\n",
    "    dsdict = dict()\n",
    "    for var in varlist:\n",
    "        if not(model == 'NorESM2' and var == 'rldscs'):\n",
    "            dsdict[var] = xr.open_dataset(glob.glob('../nc_esgf/Amon/'+var+'_Amon_'+model_filenames[model]+'_aqua4xCO2TRACMIP_r1i1p1_*')[0])\n",
    "            print('Loaded '+glob.glob('../nc_esgf/Amon/'+var+'_Amon_'+model_filenames[model]+'_aqua4xCO2TRACMIP_r1i1p1_*')[0])\n",
    "    #Assemble the different variables into one XArray Dataset\n",
    "    ds_all = xr.merge(dsdict.values())\n",
    "    #Take zonal mean\n",
    "    ds_zm = ds_all.mean(dim='lon')\n",
    "    #Regrid to the common latitude grid of the climatology file\n",
    "    #interp_like won't really work because the time dimensions don't match up. \n",
    "    #Use regular interp (but could get NaNs at boundaries)\n",
    "    ds_zm_i = ds_zm.interp(coords = {'time': ds_zm.time.data, 'lat': ds_climo.lat.data})\n",
    "    #Save NetCDF file\n",
    "    ds_zm_i.to_netcdf('nc_from_xarray/monthly_zm_forGregory_rg_a4_'+model+'.nc')\n",
    "    \n",
    "#Ultimately these were regridded in \"regrid_data_forGregory.py\" script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gregory regression is of TOA change against base state temperature. \n",
    "#So I will need that as well. But I only need the climatology for AquaControl, which I have already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_zm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK, I have now made all these monthly zonal mean files, one for each model. \n",
    "#What next? \n",
    "#Comment out above code in case I need to rerun the notebook.\n",
    "#Need to load the original climatology dataset and calculate other relevant zonal means. \n",
    "#Then do time regression (Separately for each month? Need to eliminate seasonal cycle somehow) \n",
    "#of the net TOA radiation against the climatology (load from existing dataset), to obtain forcing  \n",
    "#and feedback terms to plug into the EBM.\n",
    "#Use climatology for the ocean heat storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What about the GISS model? It's not in the climatology file. But I might be able to add it using the script adapted from Charles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all the relevant \n",
    "#(Do an example for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oh: the grids are not the same for the climatology vs. the monthly means. \n",
    "#And is it OK to run the EBM on different grids for each model? \n",
    "#There are 3 choices to deal with this: \n",
    "\n",
    "#1. Regrid the monthly means to the latitude grid of the climatology\n",
    "#2. Regrid the climatology back to the original latitude grid of each model \n",
    "#3. Download the monthly mean AquaControl data and recalculate climatology\n",
    "\n",
    "#1 seems easiest. Will have to redo the whole zonal mean loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing this--it takes about 1-2 minutes per model with the regridding (why so long?) and makes the files 3-4 times bigger.\n",
    "#NorESM2 seems to have failed at the \"merge\" stage due to a calendar issue. \n",
    "#What do the times look like for each NetCDF file for this model? Is one of them different?\n",
    "#All are \"noleap\" calendars... is it trying to merge the NorESM with the MetUM files?\n",
    "#Yes, dsdict has one variable not replaced in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load some of the others to test \n",
    "ds_rg_CAM3 = xr.open_dataset('nc_from_xarray/monthly_zm_forGregory_rg_a4_CAM3.nc')\n",
    "ds_CAM3 = xr.open_dataset('nc_from_xarray/monthly_zm_forGregory_a4_CAM3.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rg_CAM3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rg_CAM3['ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that the regridding was done correctly. Unfortunately I do seem to have nans at the poles. Duplicate these values?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rg_Nor = xr.open_dataset('nc_from_xarray/monthly_zm_forGregory_rg_a4_NorESM2.nc')\n",
    "ds_Nor = xr.open_dataset('nc_from_xarray/monthly_zm_forGregory_a4_NorESM2.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds_rg_Nor.lat, ds_rg_Nor['ts'].isel(time=0), 'k')\n",
    "plt.plot(ds_Nor.lat, ds_Nor['ts'].isel(time=0), 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looks good. Zoom in on the poles\n",
    "plt.plot(ds_rg_Nor.lat, ds_rg_Nor['ts'].isel(time=0), 'k', linewidth=3)\n",
    "plt.plot(ds_Nor.lat, ds_Nor['ts'].isel(time=0), 'g', linewidth=3, linestyle='dashed')\n",
    "plt.axis([-91, -87, 265, 267.2])\n",
    "#Still looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_rg_Nor['ts'].isel(time=0))\n",
    "#OK, NorESM has no nans. This is because the original had a data point at the poles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same plots for CAM3\n",
    "plt.plot(ds_rg_CAM3.lat, ds_rg_CAM3['ts'].isel(time=0), 'k')\n",
    "plt.plot(ds_CAM3.lat, ds_CAM3['ts'].isel(time=0), 'g')\n",
    "plt.figure()\n",
    "plt.plot(ds_rg_CAM3.lat, ds_rg_CAM3['ts'].isel(time=0), 'k', linewidth=3)\n",
    "plt.plot(ds_CAM3.lat, ds_CAM3['ts'].isel(time=0), 'g', linewidth=3, linestyle='dashed')\n",
    "#plt.axis([-91, -85, 273, 275])\n",
    "plt.axis([82, 91, 276, 277])\n",
    "#OK, original had southernmost point at -88, now cut off to -87.5 with NaNs elsewhere. \n",
    "#Similar story at north pole. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So, how to deal with the missing values? \n",
    "#Can't put NaNs into the EBM. \n",
    "#Instead \n",
    "#In the climatology file the 2 edge points pretty much duplicate each other but not the third point. \n",
    "#What is being done? Some type of cubic spline?\n",
    "#Before deciding what to do, print the regridded temperatures from every model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in models:\n",
    "#     ds_temp = xr.open_dataset('nc_from_xarray/monthly_zm_forGregory_rg_a4_'+model+'.nc')\n",
    "#     print(model)\n",
    "#     print(ds_temp.isel(time=0)['ts'])\n",
    "#     print('')\n",
    "#     print('')\n",
    "#     print('')\n",
    "    \n",
    "#How many Nans on each end for each model?\n",
    "#AM2: 1\n",
    "#CAM3: 2\n",
    "#CAM4: 0\n",
    "#CNRM: 1\n",
    "#ECHAM6.1: 1\n",
    "#ECHAM6.3: 1\n",
    "#GISS: 1\n",
    "#IPSL: 0\n",
    "#MIROC: 1\n",
    "#MPAS: 0\n",
    "#MetUM-CTL: 1\n",
    "#MetUM-ENT: 1\n",
    "#NorESM: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill all the NaNs with adjacent edge values:\n",
    "#This will require the weird XArray location indexing stuff. \n",
    "models_1nan = ['AM2', 'CNRM-AM6-DIA-v2', 'ECHAM-6.1', 'ECHAM-6.3', 'GISS-ModelE2', 'MIROC5', 'MetUM-GA6-CTL', 'MetUM-GA6-ENT']\n",
    "models_2nan = ['CAM3']\n",
    "#for model in models:\n",
    "for model in ['ECHAM-6.1', 'ECHAM-6.3']:\n",
    "    ds_temp = xr.open_dataset('nc_from_xarray/monthly_zm_forGregory_rg_a4_'+model+'.nc')\n",
    "    if model in models_1nan:\n",
    "        print('Filling in nans with adjacent values for model: ' + model)\n",
    "        #ds_temp[var].loc[dict(lat=-89.5)] = ds_temp[var].sel(lat=-88.5) #This creates error--this might only be possible in DataArrays, not Datasets.\n",
    "        #So I have to loop through the variables.\n",
    "        for var in varlist:\n",
    "            ds_temp[var].loc[dict(lat=-89.5)] = ds_temp[var].sel(lat=-88.5)\n",
    "            ds_temp[var].loc[dict(lat=89.5)] = ds_temp[var].sel(lat=88.5)\n",
    "    elif model in models_2nan:\n",
    "        print('Filling in nans with adjacent values for model: ' + model)\n",
    "        for var in varlist:\n",
    "            ds_temp[var].loc[dict(lat=-89.5)] = ds_temp[var].sel(lat=-87.5)\n",
    "            ds_temp[var].loc[dict(lat=-88.5)] = ds_temp[var].sel(lat=-87.5)\n",
    "            ds_temp[var].loc[dict(lat=88.5)] = ds_temp[var].sel(lat=87.5)\n",
    "            ds_temp[var].loc[dict(lat=89.5)] = ds_temp[var].sel(lat=87.5)\n",
    "    ds_temp.to_netcdf('nc_from_xarray/monthly_zm_forGregory_rg2_a4_'+model+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try printing again--did this work?\n",
    "\n",
    "# for model in models:\n",
    "#     ds_temp = xr.open_dataset('nc_from_xarray/monthly_zm_forGregory_rg2_a4_'+model+'.nc')\n",
    "#     print(model)\n",
    "#     print(ds_temp.isel(time=0)['ts'])\n",
    "#     print('')\n",
    "#     print('')\n",
    "#     print('')\n",
    "\n",
    "#Yes, it worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally have a set of data I can work with. Plan now: \n",
    "#Calculate Gregory regression of TOA radiation to get forcing and feedback term\n",
    "#Calculate the ocean storage term from the climatology files\n",
    "#Run perturbation EBM (first with 1.27e-6 diffusivity) to see how well this new version captures the polar amplification in TRACMIP\n",
    "\n",
    "#Then, start calculating and perturbing individual forcings/feedbacks.\n",
    "\n",
    "#Wait--still haven't figured out the issue of the seasonal cycle.\n",
    "#Subtract it first?\n",
    "#Or average it out somehow?\n",
    "#Maybe: duplicate the climatological variables to match the length of the monthly dataset, \n",
    "#       then calculate the anomaly by subtracting these. Then it's just a matter of regressing the two variables together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate Gregory regressions for total TOA radiation\n",
    "#Will run separately for each model.\n",
    "\n",
    "def GregoryNetRadiationTOA(ds_monthly, ds_climo, model):\n",
    "    #Calculate variables to regress in monthly values and climatology\n",
    "    netrad_monthly = -(ds_monthly['rlut']+ds_monthly['rsut']-ds_monthly['rsdt']) #Positive downward\n",
    "    ts_monthly = ds_monthly['ts']\n",
    "    netrad_climo = -(ds_climo['rlut']+ds_climo['rsut']-ds_climo['rsdt']).sel(exp='AquaControl', model=model).mean(dim='lon') \n",
    "    ts_climo = (ds_climo['ts']).sel(exp='AquaControl', model=model).mean(dim='lon')\n",
    "    \n",
    "    #Duplicate the climatology to match the length of the monthly output\n",
    "    numYears = int(len(ts_monthly.time)/len(ts_climo.time))\n",
    "    #How to duplicate the climatology variables? \n",
    "    #Hmm... maybe it will need to be just a regular NumPy array.\n",
    "    netrad_climo_raw = netrad_climo.data\n",
    "    ts_climo_raw = ts_climo.data\n",
    "    netrad_climo_dupe = np.tile(netrad_climo_raw, (numYears,1))\n",
    "    ts_climo_dupe = np.tile(ts_climo_raw, (numYears,1))\n",
    "    \n",
    "    #Calculate the anomaly by subtracting these variables; this will take care of seasonal cycle\n",
    "    netrad_anom = netrad_monthly - netrad_climo_dupe\n",
    "    #Try a resampling, based on XArray documentation for how to compute a seasonal mean. Hopefully calendar treated correctly.\n",
    "    t = netrad_anom.resample(time='1Y').mean('time').isel(lat=50) #Looks OK\n",
    "    ts_anom = ts_monthly - ts_climo_dupe\n",
    "    \n",
    "    #Take annual averages of the anomaly to get rid of residual seasonal cycle that is causing problems\n",
    "    #I think this time I should worry about the month lengths.\n",
    "    #Resample?\n",
    "    netrad_anom_ann = netrad_anom.resample(time='1Y').mean('time')\n",
    "    ts_anom_ann = ts_anom.resample(time='1Y').mean('time')\n",
    "    \n",
    "    #Debugging printing, plots, etc.\n",
    "    \n",
    "    print(numYears) #e.g. 40 for CAM3\n",
    "    print(np.shape(ts_climo_raw))\n",
    "    print(np.shape(ts_climo_dupe))\n",
    "    plt.figure()\n",
    "    plt.plot(netrad_anom.isel(lat=50))\n",
    "    plt.plot(np.arange(len(t))*12+6,t, 'k', linewidth=2)\n",
    "    plt.figure()\n",
    "    plt.plot(netrad_monthly.isel(lat=50))\n",
    "    plt.plot(netrad_climo_dupe[:,50])\n",
    "    plt.figure()\n",
    "    plt.plot(ts_anom.isel(lat=50))\n",
    "    plt.figure()\n",
    "    plt.plot(ts_monthly.isel(lat=50))\n",
    "    plt.plot(ts_climo_dupe[:,50])\n",
    "    \n",
    "    #Regress the net radiation anomaly against the temperature anomaly and return the slope and intercept\n",
    "    #Um... polyfit is for 1D vectors but I'm trying to run on every latitude. \n",
    "    #Could resort to a loop over the latitudes but is there an easy, vectorized way to do linear regression?\n",
    "    #No built-in function, solutions online involve pandas or sklearn.\n",
    "    #So might as well do a loop\n",
    "    #Do this for the annual average version.\n",
    "    forcing = np.zeros(len(ds_monthly.lat))\n",
    "    feedback = np.zeros(len(ds_monthly.lat))\n",
    "    for i in np.arange(len(ds_monthly.lat)):\n",
    "        p = np.polyfit(ts_anom_ann.isel(lat=i).data, netrad_anom_ann.isel(lat=i).data, 1)\n",
    "        forcing[i] = p[1] #intercept\n",
    "        feedback[i] = p[0] #slope\n",
    "    \n",
    "    return forcing, feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test for one model\n",
    "\n",
    "forcing_test, feedback_test = GregoryNetRadiationTOA(xr.open_dataset('nc_from_xarray/monthly_zm_forGregory_rg2_a4_CAM3.nc'), ds_climo, 'CAM3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forcing_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feedback_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(forcing_test)\n",
    "#Something's wrong, how can the forcing for CO2 quadrupling be negative most places? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(feedback_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From test plots I put into the function: \n",
    "#Seasonal cycle has changed between the two runs, so it still shows up and messes up the regression: so much noise. \n",
    "#What can I do? \n",
    "#Rolling averages? But this will mess up especially at the beginning..\n",
    "#Just annual averages?\n",
    "#Yes, it seems annual averages are the standard (e.g. Andrews et al., GRL, 2012). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think I've finally gotten this working. \n",
    "#Run for all the models and save as NetCDF files\n",
    "#(and then do the \"G\" term--ocean heat storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the G term: what do I need to do? \n",
    "#Change in Down - Up at surface in the climatologies,\n",
    "#for Aqua4xCO2 - AquaControl\n",
    "#G = (rlds+rsds-rlus-rsus-hfls-hfss)_Aqua4xCO2 - (same)_AquaControl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through the models, calculate forcing, feedback and ocean heat storage terms, and save them in NetCDF files via XArray datasets\n",
    "#Wait: could save just one file with all of them because they're all in the same dimensions. That would make things easier when \n",
    "#running the EBM--don't need to load files separately for each model. \n",
    "#But G term can be done outside loop\n",
    "\n",
    "G_precursor = (ds_climo.sel(exp='Aqua4xCO2')-ds_climo.sel(exp='AquaControl'))\n",
    "G_anom = G_precursor['rlds']+G_precursor['rsds']-G_precursor['rlus']-G_precursor['rsus']-G_precursor['hfls']-G_precursor['hfss']\n",
    "G_anom_zm_am = G_anom.mean(dim='lon').resample(time='1Y').mean('time')#Try resample again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_anom_zm_am2 = G_anom.mean(dim='lon').mean(dim='time') #Regular time mean--see what the differences are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(G_anom_zm_am2-G_anom_zm_am).sel(model='MIROC5').plot() #It's the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_anom_zm_am2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(G_anom_zm_am2))\n",
    "print(len(G_anom_zm_am2.lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_noGISS = ['AM2', 'CAM3', 'CAM4', 'CNRM-AM6-DIA-v2', 'ECHAM-6.1', 'ECHAM-6.3', 'IPSL-CM5A', \n",
    "              'MIROC5', 'MPAS', 'MetUM-GA6-CTL', 'MetUM-GA6-ENT', 'NorESM2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now do the loop to calculate the Gregory regressions. \n",
    "#Need DataArrays (2D arrays with model, lat coordinates) for each model.\n",
    "#Then can merge into Dataset.\n",
    "forcing = np.zeros((len(models_noGISS), len(G_anom_zm_am2.lat))) #13, 180\n",
    "feedback = np.zeros((len(models_noGISS), len(G_anom_zm_am2.lat)))\n",
    "print(forcing)\n",
    "i = 0\n",
    "for model in models_noGISS:\n",
    "    ds_temp = xr.open_dataset('nc_from_xarray/monthly_zm_forGregory_rg2_a4_'+model+'.nc')\n",
    "    forcing[i,:], feedback[i,:] = GregoryNetRadiationTOA(ds_temp, ds_climo, model)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forcing)\n",
    "print(feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK, now turn the forcing and feedback into XArray DataArrays \n",
    "#and merge them with the \"G\" term into a Dataset.\n",
    "forcing_DA = xr.DataArray(forcing, coords=[('model', models_noGISS), ('lat', G_anom_zm_am2.lat.data)], name='forcing')\n",
    "feedback_DA = xr.DataArray(feedback, coords=[('model', models_noGISS), ('lat', G_anom_zm_am2.lat.data)], name='feedback')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = G_anom_zm_am2.drop('CaltechGray', dim='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gr = G.rename('storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GregoryTotalForcingFeedbackStorage_a4 = xr.merge([forcing_DA, feedback_DA, Gr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GregoryTotalForcingFeedbackStorage_a4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GregoryTotalForcingFeedbackStorage_a4.to_netcdf('nc_from_xarray/GregoryTotalForcingFeedbackStorage_a4.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeo3.7",
   "language": "python",
   "name": "pangeo3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
